<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="abstract">Abstract</h2> <p>Using silent speech to issue commands has received growing attention, as users can utilize existing command sets from voice-based interfaces without attracting other people’s attention. Such interaction maintains privacy and social acceptance from others. However, current solutions for recognizing silent speech mainly rely on camera-based data or attaching sensors to the throat. Camera-based solutions require 5.82 times larger power consumption or have potential privacy issues; attaching sensors to the throat is not practical for commercial-off-the-shell (COTS) devices because additional sensors are required. In this paper, we propose a sensing technique that only needs a microphone and a speaker on COTS devices, which not only consumes little power but also has fewer privacy concerns. By deconstructing the received acoustic signals, a 2D motion profile can be generated. We propose a classifier based on convolutional neural networks (CNN) to identify the corresponding silent command from the 2D motion profiles. The proposed classifier can adapt to users and is robust when tested by environmental factors. Our evaluation shows that the system achieves 92.5% accuracy in classifying 20 commands.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-endophasia-use_case-480.webp 480w,/assets/img/project-endophasia-use_case-800.webp 800w,/assets/img/project-endophasia-use_case-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/project-endophasia-use_case.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 1 Use case of Endophasia. </div> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-endophasia-images-480.webp 480w,/assets/img/project-endophasia-images-800.webp 800w,/assets/img/project-endophasia-images-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/project-endophasia-images.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 2 Example of reconstructed acoustic images of various silent commands. </div> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-endophasia-system-480.webp 480w,/assets/img/project-endophasia-system-800.webp 800w,/assets/img/project-endophasia-system-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/project-endophasia-system.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 3 System flow and the proposed training model. </div> <hr> <h2 id="demo-video">Demo Video</h2> <hr> <h2 id="publication">Publication</h2> <div hidden=""> <a class="citation" href="#zhang-imwut20">Yongzhao Zhang et al., “Endophasia: Utilizing Acoustic-Based Imaging for Issuing Contact-Free Silent Speech Commands,” <i>ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</i> 4, no. 1 (2020): 37:1–37:26, https://doi.org/10.1145/3381008.</a> </div> </body></html>